# Project: Improving Healthcare Accessibility using ANN

## 1. Data Overview

This section outlines the dataset used for the project.

### Dataset Description
The dataset contains multiple independent variables related to transportation and sociodemographic factors, which are used to predict healthcare accessibility. The key variables are:

- **Dependent Variable**: `Q_19` (Healthcare Accessibility Assessment)
- **Independent Variables**:
  - `cost`: Associated costs of transportation.
  - `ability`: Physical ability to access healthcare services.
  - `safety`: Perception of safety during transit.
  - `ln_population_density`: Log of population density.
  - `ln_employment_entropy`: Log of employment entropy, representing job variety.
  - `ln_network_density`: Log of transportation network density.
  - `county_cate`: Categorical variable representing the type of county (urban, suburban, rural).
  - **Demographic Variables**:
    - `Female`: Gender (binary).
    - `Black`: Race (binary).
    - `low_income`: Income level (binary).
    - `no_vehicle`: Access to a vehicle (binary).

### Preprocessing Steps
- **One-Hot Encoding**: For categorical variables like `county_cate`, one-hot encoding is applied to convert them into numerical values.
- **Standardization**: Continuous variables are standardized to have zero mean and unit variance for better model convergence.

The dataset is then split into training (80%) and testing (20%) sets for model evaluation.

---

## 2. Experiment Setup

### Model Architecture

A simple **Artificial Neural Network (ANN)** is built to predict healthcare accessibility (`Q_19`). The model architecture consists of:
- **Input Layer**: Matching the number of independent variables after preprocessing.
- **Hidden Layers**: Two fully connected layers:
  - Layer 1: 64 neurons with ReLU activation.
  - Layer 2: 32 neurons with ReLU activation.
- **Output Layer**: Single neuron for predicting the healthcare accessibility score.

### Model Training

The model is compiled with the following parameters:
- **Optimizer**: Adam
- **Loss Function**: Mean Squared Error (MSE)
- **Training Configuration**: 
  - 50 epochs
  - Batch size of 32
  - 20% validation split during training for monitoring performance.

### Model Evaluation
- **Evaluation Metrics**: Evaluate your model on the test set using metrics like RMSE (Root Mean Square Error) or MAE (Mean Absolute Error) for regression tasks. If your task is categorical (binary or multi-class classification), use accuracy, precision, recall, F1-score, and ROC curves.
- Compare the performance of your ANN model with a baseline model, perhaps a simpler regression model or logistic regression, to highlight the improvement or benefits of using ANN.

### Experiment Code Snippet

Hereâ€™s the code that sets up the experiment in Python using TensorFlow/Keras:

```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Load the dataset
file_path = 'path_to_your_file/data_combined.txt'
data = pd.read_csv(file_path, delimiter="\t")

# Feature selection
dependent_variable = 'Q_19'
independent_variables = ['cost', 'ability', 'safety', 'ln_population_density', 'ln_employment_entropy', 
                         'ln_network_density', 'county_cate', 'Female', 'Black', 'low_income', 'no_vehicle']

X = data[independent_variables]
y = data[dependent_variable]

# One-hot encoding for 'county_cate'
X = pd.get_dummies(X, columns=['county_cate'], drop_first=True)

# Splitting the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardizing the data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Building the ANN model
model = Sequential()
model.add(Dense(64, input_dim=X_train_scaled.shape[1], activation='relu'))
model.add(Dense(32, activation='relu'))
model.add(Dense(1))  # Output layer for regression

# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error')

# Train the model
history = model.fit(X_train_scaled, y_train, validation_split=0.2, epochs=50, batch_size=32, verbose=1)

# Evaluate the model
loss = model.evaluate(X_test_scaled, y_test)

print(f'Test Loss: {loss}')
